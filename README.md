# markdown-crawler

[English](README.md) | [ç¹é«”ä¸­æ–‡](README-zhTW.md) | [æ—¥æœ¬èª](README-ja.md)

A web crawler tool optimized for AI reading that converts web content into structured Markdown format. Using intelligent algorithms to clean up noise and extract core content, it generates clean text data suitable for AI model understanding and processing. Its special feature is the ability to integrate all related pages (including the current page and all its subdirectories) into a single YAML file, producing clearly structured Markdown content.

## Why Suitable for AI Reading?

- ğŸ§  Intelligently extracts main content, removing ads, navigation bars, and other distractions
- ğŸ¯ Preserves logical structure and semantic relationships of articles
- ğŸ“‹ Converts to standardized Markdown format for easy AI parsing
- ğŸ”„ Automatically handles special characters and encoding
- ğŸ“Š Integrates all pages in YAML format for batch processing

## Features

- ğŸš€ Uses fetch and cheerio for efficient web crawling, supporting modern web pages
- ğŸ“ Uses Mozilla's Readability algorithm for intelligent content extraction
- âœ¨ Automatically converts to structured Markdown format, removing unnecessary styles and noise
- ğŸ¨ Supports GitHub Flavored Markdown (GFM), preserving important formatting
- ğŸ’» Supports syntax highlighting for code blocks, maintaining technical document readability
- ğŸ”— Automatically crawls all related pages, integrating them into a single file
- ğŸ§¹ Optimized for minimal dependencies, ensuring lightweight and fast operation
- âš¡ Significantly faster performance with optimized processing pipeline
- ğŸ“„ Enhanced Markdown output with cleaner formatting and proper link handling

## Data Integration Benefits

- ğŸ“š Automatically crawls target URL and all subdirectory pages
- ğŸ—‚ï¸ Integrates all page content into a single YAML file
- ğŸ“– Maintains the integrity of titles and content for each page
- ğŸ¯ Generates Markdown format suitable for both human reading and AI processing
- ğŸ” Facilitates quick browsing and searching of large amounts of related content

## Usage

```bash
# Basic usage
npx markdown-crawler <url> <output-filename>

# Example: Crawl website and save as output.yaml
npx markdown-crawler https://example.com output

# For URLs with spaces, use double quotes
npx markdown-crawler "https://example.com/my page" output

# Output file will automatically add .yaml extension
# Results will be saved in the current working directory
```

## Output Format

The tool integrates all related pages into a structured YAML format:
```yaml
- title: "Main Page Title"
  url: "https://example.com/"
  content: |
    # Main Page Content
    Here is the main page content...

- title: "Subpage 1 Title"
  url: "https://example.com/subpage1"
  content: |
    # Subpage 1 Content
    Here is subpage 1 content...

- title: "Subpage 2 Title"
  url: "https://example.com/subpage2"
  content: |
    # Subpage 2 Content
    Here is subpage 2 content...
```

Features:
- Automatically extracts title and main content from each page
- Includes the original URL for each page for reference
- Maintains content hierarchy and formatting
- Removes unnecessary styles and scripts
- Generates clean, readable Markdown with proper link formatting
- Preserves checkboxes and other Markdown elements correctly
- Suitable for both human reading and AI model processing

## System Requirements

- Node.js >= 16.0.0
- npm or yarn package manager

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
